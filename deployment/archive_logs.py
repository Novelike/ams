#!/usr/bin/env python3
"""
AMS Backend Log Archive Script
Archives deployment logs and old log files to free up disk space.
"""

import sys
import os
import shutil
import gzip
import logging
from datetime import datetime, timedelta
from pathlib import Path
import platform

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

class LogArchiver:
    def __init__(self):
        self.app_name = "ams-backend"
        self.is_github_actions = os.getenv("GITHUB_ACTIONS") == "true"
        self.dry_run = os.getenv("DRY_RUN", "false").lower() == "true"
        
        # Archive settings
        self.archive_days = int(os.getenv("ARCHIVE_DAYS", "30"))  # Archive logs older than 30 days
        self.keep_days = int(os.getenv("KEEP_DAYS", "90"))  # Keep archived logs for 90 days
        
        # Log directories to check
        self.log_directories = [
            "deployment/logs",
            "/var/log/ams-backend",
            "/opt/ams-backend/logs",
            "logs",
            ".github/workflows/logs"
        ]
        
        # Archive directory
        self.archive_dir = os.getenv("ARCHIVE_DIR", "deployment/archives")
        
        self.archive_results = []
        
    def log_info(self, message: str):
        logger.info(f"ğŸ“¦ {message}")
    
    def log_success(self, message: str):
        logger.info(f"âœ… {message}")
    
    def log_warning(self, message: str):
        logger.warning(f"âš ï¸ {message}")
    
    def log_error(self, message: str):
        logger.error(f"âŒ {message}")
    
    def ensure_archive_directory(self):
        """Ensure archive directory exists"""
        try:
            os.makedirs(self.archive_dir, exist_ok=True)
            self.log_success(f"ì•„ì¹´ì´ë¸Œ ë””ë ‰í† ë¦¬ ì¤€ë¹„: {self.archive_dir}")
            return True
        except Exception as e:
            self.log_error(f"ì•„ì¹´ì´ë¸Œ ë””ë ‰í† ë¦¬ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return False
    
    def find_log_files(self) -> list:
        """Find log files in various directories"""
        log_files = []
        
        for log_dir in self.log_directories:
            if os.path.exists(log_dir):
                self.log_info(f"ë¡œê·¸ ë””ë ‰í† ë¦¬ ê²€ì‚¬: {log_dir}")
                
                try:
                    log_path = Path(log_dir)
                    
                    # Find log files with common extensions
                    patterns = ["*.log", "*.log.*", "*.out", "*.err", "*.txt"]
                    
                    for pattern in patterns:
                        for log_file in log_path.rglob(pattern):
                            if log_file.is_file():
                                # Get file modification time
                                mtime = datetime.fromtimestamp(log_file.stat().st_mtime)
                                size = log_file.stat().st_size
                                
                                log_files.append({
                                    'path': str(log_file),
                                    'mtime': mtime,
                                    'size': size,
                                    'age_days': (datetime.now() - mtime).days
                                })
                
                except Exception as e:
                    self.log_warning(f"ë¡œê·¸ ë””ë ‰í† ë¦¬ ê²€ì‚¬ ì‹¤íŒ¨ {log_dir}: {str(e)}")
            else:
                self.log_info(f"ë¡œê·¸ ë””ë ‰í† ë¦¬ ì—†ìŒ: {log_dir}")
        
        self.log_info(f"ì´ {len(log_files)}ê°œ ë¡œê·¸ íŒŒì¼ ë°œê²¬")
        return log_files
    
    def archive_old_logs(self, log_files: list) -> bool:
        """Archive old log files"""
        self.log_info("ì˜¤ë˜ëœ ë¡œê·¸ íŒŒì¼ ì•„ì¹´ì´ë¸Œ ì¤‘...")
        
        archived_count = 0
        archived_size = 0
        
        # Filter files older than archive_days
        old_files = [f for f in log_files if f['age_days'] > self.archive_days]
        
        if not old_files:
            self.log_info("ì•„ì¹´ì´ë¸Œí•  ì˜¤ë˜ëœ ë¡œê·¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            self.archive_results.append("Old logs archived: 0")
            return True
        
        self.log_info(f"ì•„ì¹´ì´ë¸Œí•  íŒŒì¼ {len(old_files)}ê°œ ë°œê²¬ ({self.archive_days}ì¼ ì´ìƒ)")
        
        # Create date-based archive directory
        archive_date = datetime.now().strftime('%Y%m%d_%H%M%S')
        date_archive_dir = os.path.join(self.archive_dir, f"logs_{archive_date}")
        
        try:
            if not self.dry_run:
                os.makedirs(date_archive_dir, exist_ok=True)
            
            for log_file in old_files:
                try:
                    source_path = log_file['path']
                    file_name = os.path.basename(source_path)
                    
                    # Create compressed archive name
                    if not file_name.endswith('.gz'):
                        archive_name = f"{file_name}.gz"
                    else:
                        archive_name = file_name
                    
                    archive_path = os.path.join(date_archive_dir, archive_name)
                    
                    if not self.dry_run:
                        # Compress and archive the file
                        with open(source_path, 'rb') as f_in:
                            with gzip.open(archive_path, 'wb') as f_out:
                                shutil.copyfileobj(f_in, f_out)
                        
                        # Remove original file
                        os.remove(source_path)
                        
                        self.log_success(f"ì•„ì¹´ì´ë¸Œ ì™„ë£Œ: {file_name}")
                    else:
                        self.log_info(f"DRY RUN: {file_name} ì•„ì¹´ì´ë¸Œ ì˜ˆì •")
                    
                    archived_count += 1
                    archived_size += log_file['size']
                    
                except Exception as e:
                    self.log_error(f"íŒŒì¼ ì•„ì¹´ì´ë¸Œ ì‹¤íŒ¨ {source_path}: {str(e)}")
            
            if archived_count > 0:
                size_mb = archived_size / (1024 * 1024)
                self.log_success(f"ë¡œê·¸ ì•„ì¹´ì´ë¸Œ ì™„ë£Œ: {archived_count}ê°œ íŒŒì¼, {size_mb:.2f}MB")
                self.archive_results.append(f"Old logs archived: {archived_count} files ({size_mb:.2f}MB)")
            else:
                self.archive_results.append("Old logs archived: 0")
            
            return True
            
        except Exception as e:
            self.log_error(f"ë¡œê·¸ ì•„ì¹´ì´ë¸Œ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def cleanup_old_archives(self) -> bool:
        """Clean up old archive files"""
        self.log_info("ì˜¤ë˜ëœ ì•„ì¹´ì´ë¸Œ ì •ë¦¬ ì¤‘...")
        
        if not os.path.exists(self.archive_dir):
            self.log_info("ì•„ì¹´ì´ë¸Œ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤")
            self.archive_results.append("Old archives cleaned: 0")
            return True
        
        try:
            archive_path = Path(self.archive_dir)
            cutoff_date = datetime.now() - timedelta(days=self.keep_days)
            
            cleaned_count = 0
            cleaned_size = 0
            
            for archive_item in archive_path.iterdir():
                if archive_item.is_file() or archive_item.is_dir():
                    mtime = datetime.fromtimestamp(archive_item.stat().st_mtime)
                    
                    if mtime < cutoff_date:
                        try:
                            if archive_item.is_file():
                                size = archive_item.stat().st_size
                                if not self.dry_run:
                                    archive_item.unlink()
                                self.log_success(f"ì˜¤ë˜ëœ ì•„ì¹´ì´ë¸Œ íŒŒì¼ ì‚­ì œ: {archive_item.name}")
                            else:
                                size = sum(f.stat().st_size for f in archive_item.rglob('*') if f.is_file())
                                if not self.dry_run:
                                    shutil.rmtree(archive_item)
                                self.log_success(f"ì˜¤ë˜ëœ ì•„ì¹´ì´ë¸Œ ë””ë ‰í† ë¦¬ ì‚­ì œ: {archive_item.name}")
                            
                            cleaned_count += 1
                            cleaned_size += size
                            
                        except Exception as e:
                            self.log_error(f"ì•„ì¹´ì´ë¸Œ ì‚­ì œ ì‹¤íŒ¨ {archive_item}: {str(e)}")
            
            if cleaned_count > 0:
                size_mb = cleaned_size / (1024 * 1024)
                self.log_success(f"ì˜¤ë˜ëœ ì•„ì¹´ì´ë¸Œ ì •ë¦¬ ì™„ë£Œ: {cleaned_count}ê°œ í•­ëª©, {size_mb:.2f}MB")
                self.archive_results.append(f"Old archives cleaned: {cleaned_count} items ({size_mb:.2f}MB)")
            else:
                self.log_info("ì •ë¦¬í•  ì˜¤ë˜ëœ ì•„ì¹´ì´ë¸Œê°€ ì—†ìŠµë‹ˆë‹¤")
                self.archive_results.append("Old archives cleaned: 0")
            
            return True
            
        except Exception as e:
            self.log_error(f"ì˜¤ë˜ëœ ì•„ì¹´ì´ë¸Œ ì •ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def cleanup_large_logs(self, log_files: list) -> bool:
        """Clean up large log files"""
        self.log_info("ëŒ€ìš©ëŸ‰ ë¡œê·¸ íŒŒì¼ ì •ë¦¬ ì¤‘...")
        
        # Define large file threshold (100MB)
        large_file_threshold = 100 * 1024 * 1024
        
        large_files = [f for f in log_files if f['size'] > large_file_threshold and f['age_days'] > 1]
        
        if not large_files:
            self.log_info("ì •ë¦¬í•  ëŒ€ìš©ëŸ‰ ë¡œê·¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            self.archive_results.append("Large logs cleaned: 0")
            return True
        
        self.log_info(f"ëŒ€ìš©ëŸ‰ ë¡œê·¸ íŒŒì¼ {len(large_files)}ê°œ ë°œê²¬")
        
        cleaned_count = 0
        cleaned_size = 0
        
        for log_file in large_files:
            try:
                file_path = log_file['path']
                file_size_mb = log_file['size'] / (1024 * 1024)
                
                if not self.dry_run:
                    # Truncate large log files instead of deleting them
                    with open(file_path, 'w') as f:
                        f.write(f"# Log file truncated on {datetime.now().isoformat()}\n")
                        f.write(f"# Original size: {file_size_mb:.2f}MB\n")
                    
                    self.log_success(f"ëŒ€ìš©ëŸ‰ ë¡œê·¸ íŒŒì¼ ì •ë¦¬: {os.path.basename(file_path)} ({file_size_mb:.2f}MB)")
                else:
                    self.log_info(f"DRY RUN: {os.path.basename(file_path)} ì •ë¦¬ ì˜ˆì • ({file_size_mb:.2f}MB)")
                
                cleaned_count += 1
                cleaned_size += log_file['size']
                
            except Exception as e:
                self.log_error(f"ëŒ€ìš©ëŸ‰ ë¡œê·¸ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨ {log_file['path']}: {str(e)}")
        
        if cleaned_count > 0:
            size_mb = cleaned_size / (1024 * 1024)
            self.log_success(f"ëŒ€ìš©ëŸ‰ ë¡œê·¸ ì •ë¦¬ ì™„ë£Œ: {cleaned_count}ê°œ íŒŒì¼, {size_mb:.2f}MB")
            self.archive_results.append(f"Large logs cleaned: {cleaned_count} files ({size_mb:.2f}MB)")
        else:
            self.archive_results.append("Large logs cleaned: 0")
        
        return True
    
    def generate_archive_report(self) -> str:
        """Generate archive report"""
        self.log_info("ì•„ì¹´ì´ë¸Œ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        report = f"""
=============================================================================
AMS ë°±ì—”ë“œ ë¡œê·¸ ì•„ì¹´ì´ë¸Œ ë³´ê³ ì„œ
=============================================================================
ì•„ì¹´ì´ë¸Œ ì‹œê°„: {timestamp}
í™˜ê²½: {"GitHub Actions" if self.is_github_actions else "ì¼ë°˜ ì„œë²„"}
ëª¨ë“œ: {"DRY RUN" if self.dry_run else "ì‹¤ì œ ì‹¤í–‰"}

ì•„ì¹´ì´ë¸Œ ê²°ê³¼:
"""
        
        for result in self.archive_results:
            report += f"- {result}\n"
        
        report += f"""
ì„¤ì •:
- ì•„ì¹´ì´ë¸Œ ê¸°ì¤€: {self.archive_days}ì¼ ì´ìƒ
- ì•„ì¹´ì´ë¸Œ ë³´ê´€ ê¸°ê°„: {self.keep_days}ì¼
- ì•„ì¹´ì´ë¸Œ ë””ë ‰í† ë¦¬: {self.archive_dir}

ê²€ì‚¬í•œ ë¡œê·¸ ë””ë ‰í† ë¦¬:
"""
        
        for log_dir in self.log_directories:
            status = "ì¡´ì¬" if os.path.exists(log_dir) else "ì—†ìŒ"
            report += f"- {log_dir} ({status})\n"
        
        report += f"""
=============================================================================
"""
        
        print(report)
        self.log_success("ì•„ì¹´ì´ë¸Œ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ")
        
        return report
    
    def run_archive(self) -> int:
        """Run log archive process"""
        self.log_info("===============================================================================")
        self.log_info("ğŸ“¦ AMS ë°±ì—”ë“œ ë¡œê·¸ ì•„ì¹´ì´ë¸Œ ì‹œì‘")
        self.log_info("===============================================================================")
        
        # Ensure archive directory exists
        if not self.ensure_archive_directory():
            return 1
        
        # Find all log files
        log_files = self.find_log_files()
        
        if not log_files:
            self.log_info("ì²˜ë¦¬í•  ë¡œê·¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            self.archive_results.append("No log files found")
            self.generate_archive_report()
            return 0
        
        success_count = 0
        total_operations = 3
        
        # 1. Archive old logs
        if self.archive_old_logs(log_files):
            success_count += 1
        
        # 2. Clean up old archives
        if self.cleanup_old_archives():
            success_count += 1
        
        # 3. Clean up large logs
        if self.cleanup_large_logs(log_files):
            success_count += 1
        
        # Generate report
        self.generate_archive_report()
        
        self.log_info("===============================================================================")
        
        if success_count == total_operations:
            self.log_success("ğŸ‰ ë¡œê·¸ ì•„ì¹´ì´ë¸Œê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            return 0
        elif success_count > 0:
            self.log_warning(f"âš ï¸ ë¡œê·¸ ì•„ì¹´ì´ë¸Œê°€ ë¶€ë¶„ì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤ ({success_count}/{total_operations})")
            return 0  # Still return 0 as partial success is acceptable
        else:
            self.log_error("âŒ ë¡œê·¸ ì•„ì¹´ì´ë¸Œê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤!")
            return 1

def main():
    """Main function"""
    try:
        archiver = LogArchiver()
        exit_code = archiver.run_archive()
        sys.exit(exit_code)
        
    except KeyboardInterrupt:
        logger.error("ë¡œê·¸ ì•„ì¹´ì´ë¸Œê°€ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
        sys.exit(2)
    except Exception as e:
        logger.error(f"ë¡œê·¸ ì•„ì¹´ì´ë¸Œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        sys.exit(2)

if __name__ == "__main__":
    main()